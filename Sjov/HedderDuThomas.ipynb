{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the needed variables to be able to fecth the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "cookie = 'wfx_unq=HtDYEhZiAdNZp3y6; bizxCompanyId=semlerit; loginMethodCookieKey=SSO; assertingPartyCookieKey=afwwogyiq.accounts.ondemand.com; route=af85d2f2d68abfd87cd548ed66dd40e711863e69; bizxThemeId=9q44vxh3bw; rxVisitor=1739990854951CTPAMH53EBS2K92CJK9S0MA2OM6KE7R4; rxvt=1739999292880|1739997221512; dtPC=6$597491617_927h-vCRUFHMMORWKPIHCHQIKHGPPSOKMBENCH-0e0; dtSa=false%7C_load_%7C19%7C_onload_%7C-%7C1739997492881%7C597491617_927%7Chttps%3A%2F%2Fperformancemanager.successfactors.eu%2Fsf%2Fstart%2F%7C%7C%7C%2FcompanyCheck%7C; JSESSIONID=2E77F91FD1BEBB3E364F66C06F81CFCC.pc57bcf205; dtCookie=v_4_srv_6_sn_FA94610E03CBB9275A15696A322B76F0_perc_100000_ol_0_mul_1_app-3Ad45ce43a97ce7239_0; BIGipServerhcm57.sapsf.com=344471818.20480.0000; zsessionid=53e71f44-9be9-4f6b-aab8-28f7ee337a04'\n",
    "csrf_token = 'zkebHzP6JpEpx5Z8gcQ0HGIxbjznMIbVoyENjrAf35U%3d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make the needed definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_work_profiles(profile_id):\n",
    "    global cookie, csrf_token\n",
    "    \n",
    "    conn = http.client.HTTPSConnection(\"performancemanager.successfactors.eu\")\n",
    "    payload = ''\n",
    "    headers = {\n",
    "        'accept': 'application/schema-instance+json',\n",
    "        'accept-language': 'da,en;q=0.9,en-GB;q=0.8,en-US;q=0.7',\n",
    "        'content-type': 'application/json',\n",
    "        'cookie': cookie,\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0',\n",
    "        'x-csrf-token': csrf_token\n",
    "    }\n",
    "    url = f\"/rest/workforce/v1/workProfiles/{profile_id}?navigable=true&$select=id,displayName,displayTitle,assignmentTag,isPrimary,workerType&$expand=directReports($select=id,displayName,displayTitle,assignmentTag,isPrimary,workerType;$expand=workProfileSizeInfo($select=numberOfDirectReports,numberOfMatrixReports,numberOfMatrixSupervisorWorkProfiles,numberOfAssignments,numberOfSupervisorWorkProfiles,totalTeamSize);$top=25;$skip=0),inverseJobRelationships($expand=relatedWorkProfile($select=id,displayName,displayTitle,assignmentTag,isPrimary,workerType;$expand=workProfileSizeInfo($select=numberOfDirectReports,numberOfMatrixReports,numberOfMatrixSupervisorWorkProfiles,numberOfAssignments,numberOfSupervisorWorkProfiles,totalTeamSize));$top=25;$skip=0),workProfileSizeInfo($select=numberOfDirectReports,numberOfMatrixReports,numberOfMatrixSupervisorWorkProfiles,numberOfAssignments,numberOfSupervisorWorkProfiles,totalTeamSize)&=null\"\n",
    "    conn.request(\"GET\", url, payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    \n",
    "    # Check for new cookie and CSRF token in the response headers\n",
    "    if 'set-cookie' in res.headers:\n",
    "        new_cookie = res.headers['set-cookie']\n",
    "        if new_cookie != cookie:\n",
    "            cookie = new_cookie\n",
    "            print(f\"Updated cookie: {cookie}\")\n",
    "    \n",
    "    if 'x-csrf-token' in res.headers:\n",
    "        new_csrf_token = res.headers['x-csrf-token']\n",
    "        if new_csrf_token != csrf_token:\n",
    "            csrf_token = new_csrf_token\n",
    "            print(f\"Updated CSRF token: {csrf_token}\")\n",
    "    \n",
    "    if res.status != 200:\n",
    "        print(f\"Warning: Received HTTP {res.status} - {res.reason}\")\n",
    "        return None\n",
    "    \n",
    "    data = res.read()\n",
    "    return json.loads(data.decode(\"utf-8\"))\n",
    "\n",
    "def extract_profiles(data, parent_id=None, level=1, counter=[0]):\n",
    "    if data is None:\n",
    "        return []\n",
    "    \n",
    "    profiles = []\n",
    "    counter[0] += 1\n",
    "    profiles.append((data['id'], data['displayName'], data['displayTitle'], parent_id, level, data['workProfileSizeInfo']['numberOfDirectReports'], data['workProfileSizeInfo']['totalTeamSize']))\n",
    "    print(f\"Processing profile: {data['displayName']} (ID: {data['id']}, Level: {level}, Count: {counter[0]})\")\n",
    "    \n",
    "    if 'directReports' in data and data['workProfileSizeInfo']['numberOfDirectReports'] > 0:\n",
    "        for report in data['directReports']:\n",
    "            profiles.extend(extract_profiles(get_work_profiles(report['id']), data['id'], level + 1, counter))\n",
    "    \n",
    "    if 'inverseJobRelationships' in data:\n",
    "        for relationship in data['inverseJobRelationships']:\n",
    "            related_profile = relationship['relatedWorkProfile']\n",
    "            profiles.extend(extract_profiles(get_work_profiles(related_profile['id']), data['id'], level + 1, counter))\n",
    "    \n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set the inital node, and loop through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial call with the given profile ID\n",
    "initial_profile_id = \"31ADCB529AC2407B95B3EFC28A0DED27\"\n",
    "initial_data = get_work_profiles(initial_profile_id)\n",
    "profiles_list = extract_profiles(initial_data)\n",
    "\n",
    "# Print the total number of profiles fetched\n",
    "print(f\"Total number of profiles fetched: {len(profiles_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print the count and the result.\n",
    "\n",
    "Printing the result is a bit redundant, since we save it later in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of profiles fetched\n",
    "print(f\"Total number of profiles fetched: {len(profiles_list)}\")\n",
    "\n",
    "# Print the list of id, displayName, displayTitle, parentId, and level\n",
    "for profile_id, displayName, displayTitle, parent_id, level, numberOfDirectReports, totalTeamSize   in profiles_list:\n",
    "    print(f\"ID: {profile_id}, Name: {displayName}, Title: {displayTitle}, Parent ID: {parent_id}, Level: {level}, DirectReports: {numberOfDirectReports}, TeamSize: {totalTeamSize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then save the fetched data in `Excel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the profiles list\n",
    "df = pd.DataFrame(profiles_list, columns=['ID', 'Name', 'Title', 'Parent ID', 'Level', 'Number of Direct Reports', 'Total Team Size'])\n",
    "\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel(\"./data/profiles.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read the `Excel`file again, and split the names into Firstname, Middlenames and Lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split_name function\n",
    "def split_name(full_name):\n",
    "    name_parts = full_name.split()\n",
    "    first_name = name_parts[0]\n",
    "    last_name = name_parts[-1]\n",
    "    middle_names = \" \".join(name_parts[1:-1])\n",
    "    return first_name, middle_names, last_name\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"./data/profiles.xlsx\")\n",
    "\n",
    "# Unfortuneatly, there can be fetched duplicates in the data\n",
    "df = df.drop_duplicates(keep='first')\n",
    "\n",
    "# Apply the split_name function to the 'Name' column\n",
    "df[['First Name', 'Middle Names', 'Last Name']] = df['Name'].apply(lambda x: pd.Series(split_name(x)))\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel(\"./data/profiles_split_names.xlsx\", index=False)\n",
    "\n",
    "# Print a success message\n",
    "print(\"The names have been successfully split and saved to output/profiles_split_names.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the fun, let's try and create a wordcloud on the firstnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd # Allready imported\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the path to the Excel file\n",
    "excel_path = \"./data/profiles_split_names.xlsx\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(excel_path):\n",
    "    print(f\"File not found: {excel_path}\")\n",
    "else:\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Generate a word cloud from the 'First Name' column\n",
    "    text_first_name = \" \".join(df['First Name'].dropna().astype(str).tolist())\n",
    "    wordcloud_first_name = WordCloud(width=800, height=400, background_color='white').generate(text_first_name)\n",
    "\n",
    "    # Display the word cloud for first names\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_first_name, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Word Cloud - First Names\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generate a word cloud from the 'Title' column\n",
    "    text_title = \" \".join(df['Title'].dropna().astype(str).tolist())\n",
    "    wordcloud_title = WordCloud(width=800, height=400, background_color='white').generate(text_title)\n",
    "\n",
    "    # Display the word cloud for titles\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_title, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Word Cloud - Titles\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os lege lidt med formateringen og farverne i wordcloud'en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Define your colors\n",
    "colors = [\"#002D3F\", \"#0080B5\", \"#8DC8E8\",\"#A2AAAD\",\"#D0D3D5\",\"#D16B69\"] # Semler Colors\n",
    "\n",
    "\n",
    "# Create the colormap\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "\n",
    "# Path to your font file\n",
    "font_path = './font/GreaterTheory.otf'\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(excel_path):\n",
    "    print(f\"File not found: {excel_path}\")\n",
    "else:\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Generate a word cloud from the 'First Name' column\n",
    "    names = df[\"First Name\"].dropna().astype(str).str.strip().tolist()\n",
    "    name_count = Counter(names)\n",
    "    #text_first_name = \" \".join(df['First Name'].dropna().astype(str).tolist())\n",
    "    wordcloud_first_name = WordCloud(width=800, height=400, background_color='white',colormap=custom_cmap,font_path=font_path).generate_from_frequencies(name_count)\n",
    "\n",
    "    # Display the word cloud for first names\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_first_name, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Hvilket navn er mest populært i Semler\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generate a word cloud from the 'Title' column using unique words\n",
    "    titles = df['Title'].dropna().astype(str).str.strip().tolist()\n",
    "    title_counts = Counter(titles)\n",
    "    wordcloud_title = WordCloud(width=800, height=400, background_color='white',colormap=custom_cmap,font_path=font_path).generate_from_frequencies(title_counts)\n",
    "\n",
    "    # Display the word cloud for titles\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud_title, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Hvilken tittel er der flest af i Semler\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Lad os se på de ti mest populære navne i et søjle diagram\")\n",
    "\n",
    "    # Get the 10 most common names\n",
    "    top_10_names = name_count.most_common(10)\n",
    "\n",
    "    # Separate the names and their counts\n",
    "    names, counts = zip(*top_10_names)\n",
    "\n",
    "    # Create the bar chart\n",
    "    plt.bar(names, counts, color=custom_cmap(range(len(names))))\n",
    "    plt.xlabel('Names')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Top 10 Names')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det sidste søjlediagram er lidt presset og dårligt formateret, lad os prøve at vise det på en anden måde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data in descending order\n",
    "sorted_data = sorted(zip(counts, names), reverse=True)\n",
    "counts, names = zip(*sorted_data)\n",
    "\n",
    "# Initialize the figure\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# Remove axis\n",
    "ax.axis('off')\n",
    "\n",
    "# Create the plot\n",
    "ax.barh(names, counts, color='skyblue')\n",
    "\n",
    "for i, (value, name) in enumerate(zip(counts, names)):\n",
    "    ax.text(x=1, y=i, s=f' {name}', ha='left', va='center', fontsize=10)\n",
    "    ax.text(x=value, y=i, s=f' {int(value)}', ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det ser lidt bedre ud, men lad os ændre rækkefølgen og sætte nogle farver på:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the data in descending order\n",
    "sorted_data = sorted(zip(counts, names), reverse=False)\n",
    "counts, names = zip(*sorted_data)\n",
    "\n",
    "# Normalize the counts for colormap\n",
    "norm = plt.Normalize(min(counts), max(counts))\n",
    "colors = plt.cm.tab10(norm(counts))\n",
    "\n",
    "# Initialize the figure\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# Remove axis\n",
    "ax.axis('off')\n",
    "\n",
    "# Create the plot\n",
    "ax.barh(names, counts, color=colors)\n",
    "\n",
    "for i, (value, name) in enumerate(zip(counts, names)):\n",
    "    ax.text(x=1, y=i, s=f' {name}', ha='left', va='center', fontsize=10)\n",
    "    ax.text(x=value, y=i, s=f' {int(value)}', ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
