{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of automatic release doc maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Set initial varibles and load modules\n",
    "Before we begin, lets import the needed modules and set the variables needed to comnnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set debug level to either minimize or maximize output\n",
    "debug = 0\n",
    "\n",
    "# Import the needed modules\n",
    "import httpx\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env file\n",
    "dotenv_path = find_dotenv()\n",
    "if not dotenv_path:\n",
    "    raise FileNotFoundError(\"The .env file does not exist. Please create it and add the necessary variables.\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "organization = os.getenv('ORG')\n",
    "project = os.getenv('PRJ')\n",
    "repository = os.getenv('REP')\n",
    "personal_access_token = os.getenv('PAT')\n",
    "\n",
    "# Check if all required environment variables are set\n",
    "if not all([organization, project, repository, personal_access_token]):\n",
    "    raise EnvironmentError(\"One or more environment variables are missing. Please check your .env file.\")\n",
    "\n",
    "# Set the current and former release branch we like to compare\n",
    "current_branch_name = 'releases/release/release_20250211_163813'  \n",
    "former_branch_name = 'release/release_2024W46'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get all Pull Requests for the Release Branch\n",
    "First, you'll need to get all pull requests for your specific release branch. Here's a Python script to do that:  \n",
    "Here we get all the commits for the `current_branch_name` and the all the commits from the `former_branch_name` and the subtracts all the former commits, ending up with the commits in the latest release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits(branch_name):\n",
    "    url = f'https://dev.azure.com/{organization}/{project}/_apis/git/repositories/{repository}/commits'\n",
    "    params = {\n",
    "        'searchCriteria.itemVersion.version': branch_name,\n",
    "        'api-version': '7.1',\n",
    "        '$top': 100,\n",
    "        '$skip': 0\n",
    "    }\n",
    "\n",
    "    commits = []\n",
    "    while True:\n",
    "        response = httpx.get(url, params=params, auth=('', personal_access_token))\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.json())\n",
    "            break\n",
    "        \n",
    "        response_data = response.json()\n",
    "        \n",
    "        if 'value' not in response_data:\n",
    "            print(\"Error: 'value' key not found in the response\")\n",
    "            print(response_data)\n",
    "            break\n",
    "        \n",
    "        commits.extend(response_data['value'])\n",
    "\n",
    "        if len(response_data['value']) < params['$top']:\n",
    "            break\n",
    "\n",
    "        params['$skip'] += params['$top']\n",
    "    \n",
    "    return commits\n",
    "\n",
    "# Get commits from the former and current release branches\n",
    "former_commits = get_commits(former_branch_name)\n",
    "current_commits = get_commits(current_branch_name)\n",
    "\n",
    "# Extract commit IDs from the former release branch\n",
    "former_commit_ids = {commit['commitId'] for commit in former_commits}\n",
    "\n",
    "# Filter out commits that are already in the former release branch\n",
    "new_commits = [commit for commit in current_commits if commit['commitId'] not in former_commit_ids]\n",
    "\n",
    "# Print the new commits\n",
    "for commit in new_commits:\n",
    "    commit['date'] = commit['committer']['date']\n",
    "    if debug >= 1:\n",
    "        print(f\"Commit ID: {commit['commitId']}, Date: {commit['committer']['date']}, Message: {commit['comment']}\")\n",
    "\n",
    "df_new_commits = pd.DataFrame(new_commits)\n",
    "\n",
    "print(f\"Found {len(new_commits)} commits in {current_branch_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Work Items Related to Pull Requests\n",
    "Next, you'll need to get the work items related to each pull request. You can do this by querying the work items associated with each pull request:\n",
    "\n",
    "The trick here is to make it iterative. Since not all `commits` has a `work item` we need to loop over the `parents` to find the parent commit at see if that has work item, and if not - do it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_items = []\n",
    "commits_without_work_items = []\n",
    "\n",
    "# Regular expression to find work item IDs in the commit message\n",
    "work_item_pattern = re.compile(r'items:\\s*#(\\d+)')\n",
    "\n",
    "def find_work_items(commit, original_commit_id, organization, project, repository, personal_access_token, debug=0):\n",
    "    commit_id = commit['commitId']\n",
    "    commit_url = f'https://dev.azure.com/{organization}/{project}/_apis/git/repositories/{repository}/commits/{commit_id}?api-version=7.1'\n",
    "    commit_response = httpx.get(commit_url, auth=('', personal_access_token))\n",
    "    \n",
    "    if debug == 2:\n",
    "        print(commit_url)\n",
    "    \n",
    "    if commit_response.status_code == 200:\n",
    "        commit_details = commit_response.json()\n",
    "        comment = commit_details.get('comment', '')\n",
    "        parents = commit_details.get('parents', [])\n",
    "        \n",
    "        # Add the parent commit ID to the commit dictionary\n",
    "        if parents:\n",
    "            commit['parentCommitId'] = parents[0]  # Assuming you want the first parent if there are multiple\n",
    "        else:\n",
    "            commit['parentCommitId'] = None  # Handle case where there are no parents\n",
    "        \n",
    "        # Debug: Print commit message\n",
    "        if debug >= 2:\n",
    "            print(f\"Commit ID: {commit_id}, Message: {comment}\")\n",
    "        \n",
    "        # Find all work item IDs in the commit message\n",
    "        work_item_ids = work_item_pattern.findall(comment)\n",
    "        if work_item_ids:\n",
    "            for work_item_id in work_item_ids:\n",
    "                work_items.append({\"id\": int(work_item_id), \"commitId\": str(commit_id), \"originalCommitId\": original_commit_id})\n",
    "        else:\n",
    "            commits_without_work_items.append(commit)\n",
    "            # Recursively check parent commits\n",
    "            if commit['parentCommitId']:\n",
    "                parent_commit = {'commitId': commit['parentCommitId']}\n",
    "                find_work_items(parent_commit, original_commit_id, organization, project, repository, personal_access_token, debug)\n",
    "    else:\n",
    "        print(f\"Failed to fetch commit details for commit {commit_id}: {commit_response.text}\")\n",
    "\n",
    "# Example usage\n",
    "for commit in new_commits:\n",
    "    find_work_items(commit, commit['commitId'], organization, project, repository, personal_access_token, debug)\n",
    "\n",
    "for item in work_items:\n",
    "    if debug >= 1:\n",
    "        print(f\"Work Item ID: {item['id']}, Commit ID: {item['commitId']}, Original Commit ID: {item['originalCommitId']}\")\n",
    "\n",
    "df_work_items = pd.DataFrame(work_items)\n",
    "\n",
    "print(f\"Found {len(work_items)} work items\")\n",
    "\n",
    "# Print commits without work items\n",
    "print(f\"Found {len(commits_without_work_items)} commits without work items\")\n",
    "for commit in commits_without_work_items:\n",
    "    if debug >= 1:\n",
    "        print(f\"Commit ID: {commit['commitId']}, Date: {commit['committer']['date']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found all our `work items` lets use that information and update our original list of new commits in this release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we join the information on the commitId and originalCommitId\n",
    "df_merged = pd.merge(df_new_commits, df_work_items, left_on='commitId', right_on='originalCommitId', how='left')\n",
    "# Here we clean up the new merged dataframe, by renameing some columms amd remove redundant coloums\n",
    "df_merged = df_merged.rename(columns={'id': 'workItem','commitId_x': 'commitId', 'commitId_y': 'parrent_commitId'}).drop(['originalCommitId'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found all the `work items`, we then need to find out wheter it is a type `Story`, `Bug` or `Task`  \n",
    "However, since many `commits` resolve to fewer `work items`, lets first get a unique list of `work items` to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_work_items = [{'id': work_item} for work_item in df_merged['workItem'].unique().tolist()]\n",
    "\n",
    "updated_work_items = []\n",
    "\n",
    "for item in unique_work_items:\n",
    "    work_item_id = int(item['id'])\n",
    "    work_item_updates_url = f'https://dev.azure.com/{organization}/{project}/_apis/wit/workItems/{work_item_id}?api-version=7.1'\n",
    "    if debug >= 2:\n",
    "        print(work_item_updates_url)\n",
    "    work_item_updates_response = httpx.get(work_item_updates_url, auth=('', personal_access_token))\n",
    "    \n",
    "    if work_item_updates_response.status_code == 200:\n",
    "        try:\n",
    "            work_item = work_item_updates_response.json()\n",
    "            new_info = {\n",
    "                'Title': work_item['fields']['System.Title'],\n",
    "                'Type': work_item['fields']['System.WorkItemType'],\n",
    "                'State': work_item['fields']['System.State'],\n",
    "                'Assigned to': work_item['fields'].get('System.AssignedTo', {}).get('displayName', 'Ikke tildelt'),\n",
    "                'Area' : work_item['fields'].get('System.AreaPath')\n",
    "                }\n",
    "            item.update(new_info)\n",
    "            updated_work_items.append(item)\n",
    "        except (KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Error processing work item {work_item_id}: {e}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch work item updates for work item {work_item_id}: {work_item_updates_response.text}\")\n",
    "\n",
    "if debug >= 1:\n",
    "    for item in updated_work_items:\n",
    "        print(item)\n",
    "\n",
    "#df_workitems = pd.DataFrame(updated_work_items)\n",
    "\n",
    "print(f\"Updated {len(updated_work_items)} work items\")\n",
    "\n",
    "work_items_other = [item for item in updated_work_items if item['Type'] != 'Task']\n",
    "work_items_task =  list(filter(lambda item: item['Type'] == 'Task', updated_work_items))\n",
    "\n",
    "print(f\"{len(work_items_other)} work items are of type Story/Bug\")\n",
    "print(f\"{len(work_items_task)} work items are of type Task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is a type `Store` og `Bug` then it is fine.  \n",
    "If it is a type `Task`, then we need to find the parent `work item` and see what type than is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Find Parent Story or Bug for Each Task\n",
    "For each task, there was not a `Story` og `Bug` we need to find the parent story or bug.  \n",
    "This is a bit tricky, since we need to iterate back over the parent relation, until we find a `work item` of type `Story` og `Bug`.  \n",
    "Here we use our prior list of work item there is type `Task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = [task['id'] for task in work_items_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_work_item_updates(work_item_id):\n",
    "    url = f'https://dev.azure.com/{organization}/{project}/_apis/wit/workitems/{work_item_id}/updates'\n",
    "    params = {\n",
    "        'api-version': '7.1'\n",
    "    }\n",
    "    response = httpx.get(url, params=params, auth=('', personal_access_token))\n",
    "    return response.json()\n",
    "\n",
    "def get_parent_work_item(task_updates):\n",
    "    parent_id = None\n",
    "    for update in task_updates['value']:\n",
    "        if 'relations' in update:\n",
    "            for relation in update['relations'].get('added', []):\n",
    "                if relation['rel'] == 'System.LinkTypes.Hierarchy-Reverse':\n",
    "                    parent_url = relation['url']\n",
    "                    parent_id = parent_url.split('/')[-1]\n",
    "            for relation in update['relations'].get('removed', []):\n",
    "                if relation['rel'] == 'System.LinkTypes.Hierarchy-Reverse' and parent_id == relation['url'].split('/')[-1]:\n",
    "                    parent_id = None\n",
    "    if parent_id:\n",
    "        return get_work_item_details(parent_id)\n",
    "    return None\n",
    "\n",
    "def get_work_item_details(work_item_id):\n",
    "    url = f'https://dev.azure.com/{organization}/{project}/_apis/wit/workitems/{work_item_id}'\n",
    "    params = {\n",
    "        'api-version': '7.1'\n",
    "    }\n",
    "    response = httpx.get(url, params=params, auth=('', personal_access_token))\n",
    "    return response.json()\n",
    "\n",
    "# Extract task IDs\n",
    "task_ids = [task['id'] for task in work_items_task]\n",
    "# Fetch updates for each task\n",
    "tasks = [get_work_item_updates(task_id) for task_id in task_ids]\n",
    "# Generate parent_work_items list\n",
    "parent_work_items = [(task_id, get_parent_work_item(task)) for task_id, task in zip(task_ids, tasks)]\n",
    "\n",
    "parent_story_items = []\n",
    "\n",
    "# Print the parent Work Items\n",
    "for task_id, parent in parent_work_items:\n",
    "    if parent and 'id' in parent and 'fields' in parent:\n",
    "        parent_story_items.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"parent_id\": parent[\"id\"],\n",
    "            \"title\": parent['fields']['System.Title'],\n",
    "            \"type\": parent['fields']['System.WorkItemType'],\n",
    "            'State': parent['fields']['System.State'],\n",
    "            'Assigned to': parent['fields'].get('System.AssignedTo', {}).get('displayName', 'Ikke tildelt'),\n",
    "            'Area' : work_item['fields'].get('System.AreaPath')\n",
    "        })\n",
    "        if debug >= 1:\n",
    "            print(f\"Task ID: {task_id}, Parent ID: {parent['id']}, Title: {parent['fields']['System.Title']}, Type: {parent['fields']['System.WorkItemType']}\")\n",
    "    else:\n",
    "        print(f\"Task ID: {task_id}, No parent found or missing expected fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combining the data\n",
    "\n",
    "Let's start combining the data back together.  \n",
    "Now we have list of out `commits` and `work items`. We have found out which `work items` was of type `Story` or `Bug`, and we have found the parent `Story` or `Bug` for ther rest of the `work items` of type `Task`.\n",
    "\n",
    "This is easier to to, when using dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work_items_other = pd.DataFrame(work_items_other)\n",
    "df_parent_story_items = pd.DataFrame(parent_story_items)\n",
    "\n",
    "df_merged_1 = pd.merge(df_merged, df_work_items_other, left_on='workItem', right_on='id', how='left')\n",
    "df_merged_2 = pd.merge(df_merged_1, df_parent_story_items, left_on='workItem', right_on='task_id', how='left')\n",
    "\n",
    "# Now a bit ugly code to make the merged df look better\n",
    "# First lets combine the columns\n",
    "def combine_and_drop_columns(df, column_pairs):\n",
    "    \"\"\"\n",
    "    Combine columns and drop the redundant ones.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to modify.\n",
    "    column_pairs (list of tuples): List of tuples where each tuple contains the columns to combine.\n",
    "                                   The first column in the tuple will be kept, and the second will be dropped.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    for col1, col2 in column_pairs:\n",
    "        df[col1] = df[col1].combine_first(df[col2])\n",
    "        df.drop(columns=[col2], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Define the column pairs to combine\n",
    "column_pairs = [('id', 'task_id'), ('Title', 'title'), ('Type', 'type'),('parent_id','id'),('State_x','State_y'),('Assigned to_x','Assigned to_y'),('Area_x','Area_y')]\n",
    "\n",
    "# Combine and drop columns\n",
    "df_merged_2 = combine_and_drop_columns(df_merged_2, column_pairs)\n",
    "\n",
    "# Change the type of a column\n",
    "df_merged_2['parent_id'] = df_merged_2['parent_id'].astype('int64')\n",
    "\n",
    "# Rename the merged columns\n",
    "df_final = df_merged_2.rename(columns={'State_x':'State', 'Assigned to_x':'Assigned to', 'parent_id':'ID', 'Area_x':'Area'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's save the list as an `Excel` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel filename from branch name\n",
    "filename = './release/' + current_branch_name.split('/')[-1]\n",
    "df_final.to_excel(filename + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. let's format the result, so we better can share the result with the business.  \n",
    "Here we format the ourput to use `Markdown` but it could easily be changed to eg. `HTML` if that is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nesecary library to display Markdown\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Select the required columns and drop duplicates, and then sort on 'Type' and then by 'Story_id'\n",
    "df_release = df_final[['Type', 'ID', 'Title', 'State', 'Assigned to','Area']].drop_duplicates().sort_values(by=['Type', 'ID'])\n",
    "\n",
    "# Function to convert Story_id to hyperlink\n",
    "df_release['ID'] = df_release.apply(lambda x: f\"[{x['ID']}](https://dev.azure.com/Semler-Gruppen/Insight/_workitems/edit/{x['ID']})\", axis=1)\n",
    "\n",
    "# Split the DataFrame into Bugs and Stories\n",
    "df_bugs = df_release[df_release['Type'] == 'Bug'].drop(columns=['Type'])\n",
    "df_stories = df_release[df_release['Type'] == 'User Story'].drop(columns=['Type'])\n",
    "\n",
    "# Convert each DataFrame to Markdown format\n",
    "bugs_markdown = df_bugs.to_markdown(index=False)\n",
    "stories_markdown = df_stories.to_markdown(index=False)\n",
    "\n",
    "# Create the complex Markdown structure\n",
    "markdown_output = f\"\"\"\n",
    "# {current_branch_name}\n",
    "\n",
    "Here is the content of the last release `{current_branch_name}`\n",
    "\n",
    "## Bug\n",
    "\n",
    "{bugs_markdown}\n",
    "\n",
    "## Story\n",
    "\n",
    "{stories_markdown}\n",
    "\"\"\"\n",
    "\n",
    "# Display the Markdown output\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "1. Get all pull requests for the release branch.\n",
    "2. Get tasks related to these pull requests.\n",
    "3. Find the parent story or bug for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Markdown output to a file named release.md\n",
    "with open(filename + '.md', 'w', encoding='utf-8') as file:\n",
    "    file.write(markdown_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
